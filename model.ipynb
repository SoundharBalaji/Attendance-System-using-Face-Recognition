{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resizing Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Define the directory paths\n",
    "input_dataset_dir = r\"E:\\attendance\\faces\"# Replace with the path to your root folder\n",
    "output_dataset_dir = r\"E:\\attendance\\resized\"  # Directory where resized images will be saved\n",
    "\n",
    "# Function to resize images\n",
    "def resize_images(input_dir, output_dir, size=(160, 160)):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    for person in os.listdir(input_dir):\n",
    "        person_dir = os.path.join(input_dir, person)\n",
    "        output_person_dir = os.path.join(output_dir, person)\n",
    "        \n",
    "        if not os.path.exists(output_person_dir):\n",
    "            os.makedirs(output_person_dir)\n",
    "        \n",
    "        for img_name in os.listdir(person_dir):\n",
    "            img_path = os.path.join(person_dir, img_name)\n",
    "            try:\n",
    "                img = Image.open(img_path)\n",
    "                img_resized = img.resize(size)\n",
    "                img_resized.save(os.path.join(output_person_dir, img_name))\n",
    "            except Exception as e:\n",
    "                print(f\"Error resizing image {img_name} for person {person}: {e}\")\n",
    "\n",
    "# Call the function to resize images\n",
    "resize_images(input_dataset_dir, output_dataset_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Define augmentation transformations\n",
    "augmentation_transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Flip the image horizontally with a 50% chance\n",
    "    transforms.RandomRotation(degrees=20),   # Randomly rotate the image up to 20 degrees\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),  # Change brightness, contrast, etc.\n",
    "    transforms.RandomResizedCrop(size=(160, 160), scale=(0.8, 1.0)),  # Crop and resize\n",
    "])\n",
    "\n",
    "# Augment an image and return the augmented version\n",
    "def augment_image(image_path):\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    augmented_img = augmentation_transforms(img)\n",
    "    return augmented_img\n",
    "\n",
    "# Augment and save images to the output directory\n",
    "def augment_and_save_images(input_dir, output_dir, augment_factor=5):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    for person in os.listdir(input_dir):\n",
    "        person_dir = os.path.join(input_dir, person)\n",
    "        output_person_dir = os.path.join(output_dir, person)\n",
    "        \n",
    "        if not os.path.exists(output_person_dir):\n",
    "            os.makedirs(output_person_dir)\n",
    "        \n",
    "        for img_name in os.listdir(person_dir):\n",
    "            img_path = os.path.join(person_dir, img_name)\n",
    "            img = Image.open(img_path)\n",
    "            \n",
    "            # Save original image\n",
    "            img.save(os.path.join(output_person_dir, img_name))\n",
    "            \n",
    "            # Save augmented images\n",
    "            for i in range(augment_factor):\n",
    "                augmented_img = augment_image(img_path)\n",
    "                augmented_img.save(os.path.join(output_person_dir, f\"{img_name.split('.')[0]}_aug_{i}.jpg\"))\n",
    "\n",
    "# Augment and save dataset\n",
    "input_dataset_dir =r\"E:\\attendance\\resized\"\n",
    "output_dataset_dir =r\"E:\\attendance\\aug\"\n",
    "augment_and_save_images(input_dataset_dir, output_dataset_dir, augment_factor=4)  # Augment each image 3 times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_augmented_dataset = r\"E:\\attendance\\aug\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the complete transformation (resizing, optional augmentations)\n",
    "complete_transform = transforms.Compose([\n",
    "    transforms.Resize((250, 250)),  # Resize to 250x250 pixels\n",
    "    transforms.RandomHorizontalFlip(),  # Randomly flip images\n",
    "    transforms.ToTensor(),           # Convert image to tensor\n",
    "])\n",
    "\n",
    "# Create the dataset using the dataset directory\n",
    "dataset = datasets.ImageFolder(root=output_dataset_dir, transform=complete_transform)\n",
    "\n",
    "# Create a DataLoader\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Check the number of classes in the dataset\n",
    "num_classes = len(dataset.classes)\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# Visualize a batch of images\n",
    "def show_images(images, labels):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    num_images = min(len(images), 10)  # Handle fewer than 10 images\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(2, 5, i + 1)\n",
    "        plt.imshow(images[i].permute(1, 2, 0))  # Permute dimensions for visualization\n",
    "        plt.title(dataset.classes[labels[i]])\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Inspecting a batch of images\n",
    "images, labels = next(iter(data_loader))\n",
    "show_images(images, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings and Knn Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "from PIL import Image\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Initialize device, MTCNN, and InceptionResnetV1\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "mtcnn = MTCNN(keep_all=False, device=device)\n",
    "model = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
    "\n",
    "def get_embeddings(image_path):\n",
    "    \"\"\"Extract embeddings from a given image path.\"\"\"\n",
    "    try:\n",
    "        img = Image.open(image_path).convert(\"RGB\")\n",
    "        boxes, _ = mtcnn.detect(img)\n",
    "        if boxes is not None:\n",
    "            faces = mtcnn(img)\n",
    "            faces = faces.to(device)\n",
    "            embeddings = model(faces.unsqueeze(0))\n",
    "            return embeddings.detach().cpu().numpy()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "    return None\n",
    "\n",
    "def process_directory(input_dir):\n",
    "    \"\"\"Process images in a directory to extract embeddings.\"\"\"\n",
    "    embeddings_dict = {}\n",
    "    \n",
    "    for person in os.listdir(input_dir):\n",
    "        person_dir = os.path.join(input_dir, person)\n",
    "        if not os.path.isdir(person_dir):\n",
    "            continue\n",
    "        \n",
    "        embeddings_list = []\n",
    "        for img_name in os.listdir(person_dir):\n",
    "            img_path = os.path.join(person_dir, img_name)\n",
    "            embeddings = get_embeddings(img_path)\n",
    "            if embeddings is not None:\n",
    "                embeddings_list.append(embeddings)\n",
    "        \n",
    "        if embeddings_list:\n",
    "            embeddings_dict[person] = np.vstack(embeddings_list)\n",
    "\n",
    "    return embeddings_dict\n",
    "\n",
    "def train_knn_classifier(embeddings_dict):\n",
    "    \"\"\"Train KNN classifier on the extracted embeddings.\"\"\"\n",
    "    X, y = [], []\n",
    "    for person, embeddings in embeddings_dict.items():\n",
    "        X.append(embeddings)\n",
    "        y.extend([person] * embeddings.shape[0])\n",
    "\n",
    "    X = np.vstack(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # Split data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    k_values = range(1, 21)\n",
    "    accuracies = []\n",
    "\n",
    "    for k in k_values:\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        knn.fit(X_train, y_train)\n",
    "        accuracies.append(knn.score(X_test, y_test))\n",
    "\n",
    "# Plot the accuracy\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(k_values, accuracies, marker='o')\n",
    "    plt.title('K-Value vs. Accuracy')\n",
    "    plt.xlabel('Number of Neighbors (k)')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xticks(k_values)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # Train KNN classifier\n",
    "    knn = KNeighborsClassifier(n_neighbors=8)\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    return knn, X_test, y_test\n",
    "\n",
    "def evaluate_model(knn, X_test, y_test):\n",
    "    \"\"\"Evaluate the KNN model and display metrics.\"\"\"\n",
    "    y_pred = knn.predict(X_test)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title(f'Confusion Matrix (Accuracy: {accuracy:.2f})')\n",
    "    plt.show()\n",
    "\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Main execution\n",
    "input_dir = r\"E:\\attendance\\aug\"\n",
    "embeddings_dict = process_directory(input_dir)\n",
    "knn, X_test, y_test = train_knn_classifier(embeddings_dict)\n",
    "evaluate_model(knn, X_test, y_test)\n",
    "\n",
    "# Save the classifier\n",
    "joblib.dump(knn, 'allknn.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
